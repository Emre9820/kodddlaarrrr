import os
import numpy as np
import pandas as pd
import joblib
from collections import defaultdict
import matplotlib.pyplot as plt # Grafik çizimi için eklendi

from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import accuracy_score, classification_report # accuracy_score eklendi
from sklearn.model_selection import train_test_split

import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import torch.nn.functional as F # Softmax için eklendi
from torch.optim.lr_scheduler import ReduceLROnPlateau # Scheduler için eklendi

# Reproducibility için seed ayarı
np.random.seed(42)
torch.manual_seed(42)

###############################################
# SNV Uygulama Fonksiyonu - YENİ EKLENDİ
###############################################
def snv(data):
    """
    Standard Normal Variate (SNV) dönüşümü uygular.

    Args:
        data: NumPy dizisi (örnekler x özellikler)

    Returns:
        SNV uygulanmış NumPy dizisi
    """
    data = np.array(data)
    # Her satır (örnek) için ortalama ve standart sapma hesapla
    mean = np.mean(data, axis=1, keepdims=True)
    std = np.std(data, axis=1, keepdims=True)
    # SNV uygula (sıfır standart sapma durumunda sıfır bölme hatasını önle)
    snv_data = np.where(std == 0, 0, (data - mean) / std)
    return snv_data

###############################################
# Veri Yükleme Fonksiyonu (Sadece _a.csv)
###############################################
def load_data(data_dir, mode='train'):
    """
    Verileri yükler ve özellik çıkarımı yapar (sadece _a.csv).
    """
    data_a = [] # _a.csv verisi için
    labels = []  # Eğitim için gerçek etiketler (ana/alt)
    file_labels = []  # Tahmin modunda gerçek etiket bilgisini saklamak için

    for main_folder in os.listdir(data_dir):
        main_class_path = os.path.join(data_dir, main_folder)
        if not os.path.isdir(main_class_path):
            continue

        for path, _, _ in os.walk(main_class_path):
            if path == main_class_path:
                continue

            for file_name in os.listdir(path):
                # Sadece _a.csv dosyalarını oku
                if file_name.endswith("_a.csv"):
                    try:
                        file_path = os.path.join(path, file_name)
                        df_a = pd.read_csv(file_path, skiprows=28)
                        # İkinci sütundaki verileri al (Absorbance (AU))
                        feat_a = df_a.iloc[:, 1].values.flatten()
                        data_a.append(feat_a)

                        true_label = os.path.join(
                            os.path.basename(os.path.dirname(path)),
                            os.path.basename(path)
                        )
                        if mode == 'train':
                            labels.append(true_label)
                        else:
                            file_labels.append(true_label)
                    except Exception as e:
                        print(f"Hata: {file_name} işlenirken hata oluştu: {e}. Atlanıyor.")

    if mode == 'train':
        return np.array(data_a), labels
    else:
        return np.array(data_a), file_labels

###############################################
# CNN Model Tanımı - GÜNCELLENDİ
###############################################
class CNN(nn.Module):
    def __init__(self, input_size, num_classes):
        super(CNN, self).__init__()
        self.conv1 = nn.Conv1d(1, 32, kernel_size=3, stride=1, padding=1)
        self.relu1 = nn.ReLU()
        self.conv2 = nn.Conv1d(32, 64, kernel_size=3, stride=1, padding=1)
        self.relu2 = nn.ReLU()
        self.pool1 = nn.MaxPool1d(kernel_size=2, stride=2)
        self.conv3 = nn.Conv1d(64, 128, kernel_size=3, stride=1, padding=1)
        self.relu3 = nn.ReLU()
        self.pool2 = nn.MaxPool1d(kernel_size=2, stride=2)
        self.conv4 = nn.Conv1d(128, 256, kernel_size=3, stride=1, padding=1)
        self.relu4 = nn.ReLU()
        self.pool3 = nn.MaxPool1d(kernel_size=2, stride=2)
        # Pooling sonrası boyutu dinamik olarak hesapla
        self.fc1 = nn.Linear(256 * (input_size // 8), 512)
        self.ln1 = nn.LayerNorm(512)
        self.relu5 = nn.ReLU()
        self.dropout = nn.Dropout(p=0.3)
        self.fc2 = nn.Linear(512, num_classes)

    def forward(self, x):
        # Girdi x: (batch_size, 1, input_size) olmalı
        x = self.relu1(self.conv1(x))
        x = self.relu2(self.conv2(x))
        x = self.pool1(x)
        x = self.relu3(self.conv3(x))
        x = self.pool2(x)
        x = self.relu4(self.conv4(x))
        x = self.pool3(x)
        # Flatten işlemi: (batch_size, channels, length) -> (batch_size, channels * length)
        x = x.view(x.size(0), -1)
        x = self.fc1(x)
        x = self.ln1(x)
        x = self.relu5(x)
        x = self.dropout(x)
        x = self.fc2(x)
        return x

###############################################
# PyTorch Dataset Tanımı - GÜNCELLENDİ (CNN için boyut ekleme)
###############################################
class SpectralDataset(Dataset):
    def __init__(self, X, y):
        self.X = X # numpy array (num_samples, num_features)
        self.y = y # numpy array (num_samples,)

    def __len__(self):
        return len(self.X)

    def __getitem__(self, idx):
        # CNN Conv1d (batch, channels, length) bekler. Channel=1 ekle.
        sample = torch.tensor(self.X[idx], dtype=torch.float32).unsqueeze(0)
        label = torch.tensor(self.y[idx], dtype=torch.long)
        return sample, label


###############################################
# Tahmin Sonuçlarını Grup Bazında Raporlama (Değişiklik yok)
###############################################
def print_grouped_predictions_summary(predictions):
    import pandas as pd
    df = pd.DataFrame(predictions, columns=['Gerçek', 'Tahmin', 'Olasılık'])

    grouped_true = df.groupby('Gerçek')
    total_correct_predictions = 0
    total_wrong_predictions = 0

    for true_label, group in grouped_true:
        total = len(group)
        true_main = true_label.split('/')[0]

        # Doğru tahmin: tahmin edilen etiketin ana kısmı gerçek ana ile eşleşiyorsa
        correct_mask = group['Tahmin'].apply(lambda x: x.split('/')[0] == true_main)
        correct_group = group[correct_mask]
        correct_count = len(correct_group)
        accuracy = (correct_count / total) * 100 if total > 0 else 0

        print(f"Gerçek: {true_label}")
        print(f"  Doğru tahminler ({true_main}): {correct_count} dosya, Doğruluk: %{accuracy:.2f}")

        total_correct_predictions += correct_count
        total_wrong_predictions += (total - correct_count)

        wrong_group = group[~correct_mask]
        if not wrong_group.empty:
            # Yanlış tahminleri alt klasörleriyle birlikte grupla
            wrong_summary = wrong_group.groupby('Tahmin').agg(
                count=('Tahmin', 'count'),
                avg_prob=('Olasılık', 'mean')
            ).reset_index()
            wrong_summary = wrong_summary.sort_values(by='count', ascending=False).head(3)

            print("  Yanlış tahminler:")
            for _, row in wrong_summary.iterrows():
                print(f"    {row['Tahmin']}: {row['count']} dosya, Ortalama Olasılık: {row['avg_prob']:.4f}")
        print("")

    total_predictions = len(predictions)
    overall_accuracy = (total_correct_predictions / total_predictions) * 100 if total_predictions > 0 else 0
    print(
        f"Toplamda {total_predictions} prediction verisinden, {total_correct_predictions} doğru, {total_wrong_predictions} yanlış veri var. Doğruluk oranı: %{overall_accuracy:.2f}")

def print_class_based_accuracy(predictions):
    """
    Her sınıf için doğru ve yanlış tahmin sayısını ve doğruluğunu hesaplar ve yazdırır.
    """
    import pandas as pd
    df = pd.DataFrame(predictions, columns=['Gerçek', 'Tahmin', 'Olasılık'])

    all_classes = set(df['Gerçek'].unique())
    all_classes.update(df['Tahmin'].unique())

    for class_name in sorted(all_classes):

        class_df = df[df['Gerçek'] == class_name]
        if class_df.empty:
            continue

        total_samples = len(class_df)
        # Doğru tahmin: Gerçek etiket ile Tahmin etiketi aynı olanlar
        correct_predictions = len(class_df[class_df['Gerçek'] == class_df['Tahmin']])

        accuracy = (correct_predictions / total_samples) * 100 if total_samples > 0 else 0

        print(f"Sınıf: {class_name}")
        print(f"  Toplam örnek: {total_samples}")
        print(f"  Doğru tahmin: {correct_predictions}")
        print(f"  Yanlış tahmin: {total_samples - correct_predictions}")
        print(f"  Doğruluk: %{accuracy:.2f}")
        print("")

###############################################
# Ana Fonksiyon: Eğitim ve Tahmin (CNN) - GÜNCELLENDİ
###############################################
def main():
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"Kullanılan cihaz: {device}")

    ##############################
    # 1. Eğitim
    ##############################
    TRAIN_DIR = "/home/han/Documents/500_veri/Train"

    print("Eğitim verisi yükleniyor...")
    # Sadece _a.csv verisini yükle
    data_a, train_labels = load_data(TRAIN_DIR, mode='train')
    if data_a.size == 0:
        print("Hata: Eğitim verisi yüklenemedi veya hiç _a.csv dosyası bulunamadı.")
        return
    print(f"Eğitim verisi boyutu (orijinal): {data_a.shape}\n")

    # Tüm veriyi %80 eğitim, %20 doğrulama olarak ayırma
    # Stratify kullanarak sınıfların oranını koru
    try:
        data_a_train, data_a_val, train_labels_train, train_labels_val = train_test_split(
            data_a, train_labels, test_size=0.2, random_state=42, stratify=train_labels
        )
    except ValueError as e:
        print(f"train_test_split hatası: {e}. Sınıf sayısı az olabilir, stratify olmadan deneniyor.")
        data_a_train, data_a_val, train_labels_train, train_labels_val = train_test_split(
            data_a, train_labels, test_size=0.2, random_state=42
        )

    # SNV Uygula - YENİ EKLENDİ
    print("Eğitim ve doğrulama verilerine SNV uygulanıyor...")
    data_a_train_snv = snv(data_a_train)
    data_a_val_snv = snv(data_a_val)
    print(f"SNV sonrası eğitim verisi boyutu: {data_a_train_snv.shape}")
    print(f"SNV sonrası doğrulama verisi boyutu: {data_a_val_snv.shape}\n")

    # Etiket kodlama (ana/alt şeklinde)
    y_encoder = LabelEncoder()
    y_train = y_encoder.fit_transform(train_labels_train)
    y_val = y_encoder.transform(train_labels_val)
    joblib.dump(y_encoder, "label_encoder_cnn_snv.pkl") # Farklı isimle kaydet

    # Dataset oluşturma (SNV uygulanmış veri ile) - GÜNCELLENDİ
    train_dataset = SpectralDataset(data_a_train_snv, y_train)
    val_dataset = SpectralDataset(data_a_val_snv, y_val)
    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)

    # CNN modelini oluştur - GÜNCELLENDİ
    input_size = data_a_train_snv.shape[1] # Özellik sayısı (SNV sonrası değişmez)
    num_classes = len(np.unique(y_train))
    model = CNN(input_size, num_classes).to(device)

    # Loss fonksiyonu, optimizer ve LR scheduler - GÜNCELLENDİ
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.AdamW(model.parameters(), lr=0.01)
    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5, verbose=True)

    # Eğitim parametreleri ve Early Stopping için değişkenler - GÜNCELLENDİ
    epochs = 5000 # Maksimum epoch sayısı
    early_stop_patience = 10 # Kaç epoch iyileşme olmazsa durdurulacak
    early_stop_counter = 0
    best_val_loss = float('inf')
    best_epoch = -1
    best_train_loss = float('inf')
    best_train_acc = 0.0
    best_val_acc = 0.0

    # Grafik için listeler
    train_losses = []
    val_losses = []
    train_accuracies = []
    val_accuracies = []

    print("CNN modeli (SNV ile) eğitiliyor...\n")
    for epoch in range(epochs):
        # --- Eğitim Aşaması ---
        model.train()
        running_train_loss = 0.0
        correct_train = 0
        total_train = 0
        for inputs, labels in train_loader:
            # inputs boyutu: [batch_size, 1, input_size]
            # labels boyutu: [batch_size]
            inputs, labels = inputs.to(device), labels.to(device)

            optimizer.zero_grad()
            outputs = model(inputs) # Model forward çağrısı
            loss = criterion(outputs, labels)
            loss.backward()
            optimizer.step()

            running_train_loss += loss.item() * inputs.size(0)
            _, predicted = torch.max(outputs.data, 1)
            total_train += labels.size(0)
            correct_train += (predicted == labels).sum().item()

        epoch_train_loss = running_train_loss / total_train
        epoch_train_acc = correct_train / total_train * 100
        train_losses.append(epoch_train_loss)
        train_accuracies.append(epoch_train_acc)

        # --- Doğrulama Aşaması ---
        model.eval()
        running_val_loss = 0.0
        correct_val = 0
        total_val = 0
        with torch.no_grad():
            for inputs, labels in val_loader:
                inputs, labels = inputs.to(device), labels.to(device)
                outputs = model(inputs)
                loss = criterion(outputs, labels)
                running_val_loss += loss.item() * inputs.size(0)
                _, predicted = torch.max(outputs.data, 1)
                total_val += labels.size(0)
                correct_val += (predicted == labels).sum().item()

        epoch_val_loss = running_val_loss / total_val
        epoch_val_acc = correct_val / total_val * 100
        val_losses.append(epoch_val_loss)
        val_accuracies.append(epoch_val_acc)

        print(f'Epoch [{epoch+1}/{epochs}] - Train Loss: {epoch_train_loss:.4f}, Train Acc: {epoch_train_acc:.2f}% | Val Loss: {epoch_val_loss:.4f}, Val Acc: {epoch_val_acc:.2f}%')

        # LR Scheduler'ı güncelle
        scheduler.step(epoch_val_loss)

        # --- Early Stopping ve En İyi Modeli Kaydetme ---
        # GÜNCELLENDİ: Durumun iyileşip iyileşmediğini kontrol et (daha küçük loss daha iyi)
        if epoch_val_loss < best_val_loss:
            best_val_loss = epoch_val_loss
            best_epoch = epoch + 1
            best_train_loss = epoch_train_loss # En iyi epoch'taki train loss
            best_train_acc = epoch_train_acc   # En iyi epoch'taki train acc
            best_val_acc = epoch_val_acc     # En iyi epoch'taki val acc
            early_stop_counter = 0 # İyileşme oldu, sayacı sıfırla
            # En iyi modeli kaydet - GÜNCELLENDİ (dosya adı)
            torch.save(model.state_dict(), "best_cnn_snv_model.pth")
            print(f"Epoch {epoch+1}: Validation loss düştü ({best_val_loss:.4f}). Model kaydedildi.")
        else:
            early_stop_counter += 1 # İyileşme olmadı (aynı kaldı veya arttı), sayacı artır
            print(f"Epoch {epoch+1}: Validation loss düşmedi veya aynı kaldı. Sayaç: {early_stop_counter}/{early_stop_patience}")
            if early_stop_counter >= early_stop_patience:
                print("Erken durdurma tetiklendi (Validation loss iyileşmedi)!")
                break # Eğitim döngüsünü sonlandır

    print(f"\nEğitim tamamlandı.")
    print(f"En iyi epoch: {best_epoch} - Train Loss: {best_train_loss:.4f}, Train Acc: {best_train_acc:.2f}% | Val Loss: {best_val_loss:.4f}, Val Acc: {best_val_acc:.2f}%")
    print("En iyi model 'best_cnn_snv_model.pth' olarak kaydedildi.\n")

    # --- Grafik Çizimi ---
    plt.figure(figsize=(12, 5))

    plt.subplot(1, 2, 1)
    plt.plot(range(1, len(train_losses) + 1), train_losses, label='Train Loss')
    plt.plot(range(1, len(val_losses) + 1), val_losses, label='Validation Loss')
    if best_epoch != -1: # Eğer model hiç kaydedilmediyse hata vermesin
        plt.axvline(best_epoch, linestyle='--', color='r', label=f'Best Epoch ({best_epoch})')
    plt.xlabel('Epochs')
    plt.ylabel('Loss')
    plt.title('Training and Validation Loss (with SNV)')
    plt.legend()
    plt.grid(True)

    plt.subplot(1, 2, 2)
    plt.plot(range(1, len(train_accuracies) + 1), train_accuracies, label='Train Accuracy')
    plt.plot(range(1, len(val_accuracies) + 1), val_accuracies, label='Validation Accuracy')
    if best_epoch != -1:
        plt.axvline(best_epoch, linestyle='--', color='r', label=f'Best Epoch ({best_epoch})')
    plt.xlabel('Epochs')
    plt.ylabel('Accuracy (%)')
    plt.title('Training and Validation Accuracy (with SNV)')
    plt.legend()
    plt.grid(True)

    plt.tight_layout()
    plt.show()


    ##############################
    # 2. Tahmin (En İyi Modeli Kullanarak)
    ##############################
    SAMPLE_DIR = "/home/han/Documents/Prediction"
    print("\nTahmin verileri yükleniyor...")
    try:
        # Sadece _a.csv yükle
        data_a_pred, true_labels = load_data(SAMPLE_DIR, mode='predict')
        if data_a_pred.size == 0:
             print("Hata: Tahmin verisi yüklenemedi veya hiç _a.csv dosyası bulunamadı.")
             return
        print(f"Tahmin verisi boyutu (orijinal): {data_a_pred.shape}")

        # SNV Uygula - YENİ EKLENDİ
        print("Tahmin verilerine SNV uygulanıyor...")
        data_a_pred_snv = snv(data_a_pred)
        print(f"SNV sonrası tahmin verisi boyutu: {data_a_pred_snv.shape}\n")

        # Tahmin için Dataset ve DataLoader (SNV uygulanmış veri ile) - GÜNCELLENDİ
        pred_dataset = SpectralDataset(data_a_pred_snv, np.zeros(len(data_a_pred_snv)))
        pred_loader = DataLoader(pred_dataset, batch_size=1, shuffle=False)

        # En iyi modeli yükle - GÜNCELLENDİ (model ve dosya adı)
        print("En iyi model ('best_cnn_snv_model.pth') yükleniyor...")
        # Model mimarisini tekrar tanımla ve kaydedilen ağırlıkları yükle
        model_pred = CNN(input_size, num_classes).to(device)
        model_pred.load_state_dict(torch.load("best_cnn_snv_model.pth", map_location=device))
        model_pred.eval() # Modeli değerlendirme moduna al
        y_encoder = joblib.load("label_encoder_cnn_snv.pkl") # Kaydedilen encoder'ı yükle

        # Tahmin yap ve olasılıkları al
        predictions = []
        with torch.no_grad():
            for idx, (inputs, _) in enumerate(pred_loader):
                # inputs boyutu: [1, 1, input_size]
                inputs = inputs.to(device)
                outputs = model_pred(inputs)
                # Olasılıkları almak için softmax uygula
                probs = F.softmax(outputs, dim=1).cpu().numpy().flatten()
                pred_idx = np.argmax(probs)
                pred_class = y_encoder.inverse_transform([pred_idx])[0]
                pred_prob = float(probs[pred_idx]) # En yüksek olasılık değeri
                true_label = true_labels[idx]
                predictions.append((true_label, pred_class, pred_prob))

        print("\n--- Tahmin Sonuçları (SNV ile) ---")
        print_grouped_predictions_summary(predictions)
        print("\n--- Sınıf Bazlı Doğruluk (SNV ile) ---")
        print_class_based_accuracy(predictions)
        print("\nTahmin işlemi başarıyla tamamlandı!")

    except FileNotFoundError:
        # GÜNCELLENDİ (dosya adları)
        print(f"Hata: 'best_cnn_snv_model.pth' veya 'label_encoder_cnn_snv.pkl' bulunamadı. Lütfen önce modeli eğitin.")
    except Exception as e:
        print(f"Tahmin sırasında bir hata oluştu: {str(e)}")

if __name__ == "__main__":
    main()

